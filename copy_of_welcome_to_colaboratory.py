# -*- coding: utf-8 -*-
"""Copy of Welcome To Colaboratory

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bAgqmcdoG5h0oHgmpek2cloCG_9lSEUx

<div class="markdown-google-sans">

## Machine learning
</div>

With Colab you can import an image dataset, train an image classifier on it, and evaluate the model, all in just [a few lines of code](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb). Colab notebooks execute code on Google's cloud servers, meaning you can leverage the power of Google hardware, including [GPUs and TPUs](#using-accelerated-hardware), regardless of the power of your machine. All you need is a browser.

Colab is used extensively in the machine learning community with applications including:
- Getting started with TensorFlow
- Developing and training neural networks
- Experimenting with TPUs
- Disseminating AI research
- Creating tutorials

To see sample Colab notebooks that demonstrate machine learning applications, see the [machine learning examples](#machine-learning-examples) below.

<div class="markdown-google-sans">

## More Resources

### Working with Notebooks in Colab

</div>

- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)
- [Guide to Markdown](/notebooks/markdown_guide.ipynb)
- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)
- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)
- [Interactive forms](/notebooks/forms.ipynb)
- [Interactive widgets](/notebooks/widgets.ipynb)

<div class="markdown-google-sans">

<a name="working-with-data"></a>
### Working with Data
</div>

- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)
- [Charts: visualizing data](/notebooks/charts.ipynb)
- [Getting started with BigQuery](/notebooks/bigquery.ipynb)

<div class="markdown-google-sans">

### Machine Learning Crash Course

<div>

These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.
- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)
- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)

<div class="markdown-google-sans">

<a name="using-accelerated-hardware"></a>
### Using Accelerated Hardware
</div>

- [TensorFlow with GPUs](/notebooks/gpu.ipynb)
- [TensorFlow with TPUs](/notebooks/tpu.ipynb)

<div class="markdown-google-sans">

<a name="machine-learning-examples"></a>

### Featured examples

</div>

- [NeMo Voice Swap](https://colab.research.google.com/github/NVIDIA/NeMo/blob/stable/tutorials/VoiceSwapSample.ipynb): Use Nvidia's NeMo conversational AI Toolkit to swap a voice in an audio fragment with a computer generated one.

- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.
- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.
- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.
- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.
- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import KFold
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import matplotlib.pyplot as plt
import re

import pandas as pd
df = pd.read_csv('tweetsdatasetnewall.csv')

df.head(5)

df.dtypes

df.columns

df = df.drop(['handle', 'name', 'replies', 'retweets', 'favorite',
       'unix_timestamp', 'date', 'url', 'search_url', 'hashtags',
       'extracted_hashtags', 'clean_tweet', 'sentiment_results'], axis = 1)

df.head(10)

"""Preprocess of text data"""

nltk.download('punkt')
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    text = re.sub(r'http\S+', '', text)     # Remove links
    text = ' '.join([word for word in text.split() if not word.startswith('@')])  # Remove mentions (words starting with '@')
    text = re.sub(r'\$\S+', '', text) # Remove currency symbols
    text = re.sub(r'\d{10}', '', text)  # Remove contact numbers
    text = re.sub(r'\d{1,2}[-/]\d{1,2}[-/]\d{2,4}', '', text) # Remove dates
    text = re.sub(r'\S+@\S+', '', text) # Remove emails
    text = re.sub(r'\d+\s\w+\s\w+', '', text) # Remove street addresses
    text = re.sub(r'\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}', '', text)  # Remove credit card numbers

    # Remove short words (length <= 2)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if len(word) > 2]

    # Tokenization, lowercase, and removing stopwords
    tokens = [word.lower() for word in tokens if word.isalnum()]
    tokens = [word for word in tokens if word not in stop_words]

    return ' '.join(tokens)

df['processed_content'] = df['content'].apply(preprocess_text)

df.head(10)

"""Feature Extraction"""

# Convert text to CountVectorizer features
vectorizer = CountVectorizer()
X_count = vectorizer.fit_transform(df['processed_content'])
y = df['sentiment']

# Get the vocabulary and word counts
vocabulary = vectorizer.get_feature_names_out()
word_counts = X_count.sum(axis=0).A1  # Convert to array

# Create a dictionary of words and their corresponding counts
word_freq = dict(zip(vocabulary, word_counts))

# Sort the dictionary by counts in descending order
sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

# Display the top most used words
num_top_words = 20
print("Most Used Words:")
for word, freq in sorted_word_freq[:num_top_words]:
    print(f"{word}: {freq}")

num_top_words = 20
top_words = [word for word, freq in sorted_word_freq[:num_top_words]]
top_freqs = [freq for word, freq in sorted_word_freq[:num_top_words]]


plt.figure(figsize=(10, 6))
plt.barh(top_words, top_freqs, color='skyblue')
plt.xlabel('Word Frequency')
plt.ylabel('Words')
plt.title('Most Used Words')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_count, y, test_size=0.2, random_state=42)

"""Define the SVM classifier"""

svm_classifier = SVC(random_state=42)

# Hyperparameter grid to search
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

# Grid search with cross-validation on the training data
grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid, cv=3)
grid_search.fit(X_train, y_train)

# Best hyperparameters from the grid search
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Train the final model with the best hyperparameters on the full training dataset
final_svm_classifier = SVC(**best_params, random_state=42)
final_svm_classifier.fit(X_train, y_train)

# Predict sentiment on the testing dataset
y_pred = final_svm_classifier.predict(X_test)

# Evaluate the final model on the testing dataset
report = classification_report(y_test, y_pred)
print("Classification Report on Testing Data:")
print(report)

"""Define the Decision Tree classifier"""

from sklearn.tree import DecisionTreeClassifier

dt_classifier = DecisionTreeClassifier(random_state=42)

# Hyperparameter grid to search
param_grid_dt = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Grid search with cross-validation on the training data
grid_search_dt = GridSearchCV(estimator=dt_classifier, param_grid=param_grid_dt, cv=3)
grid_search_dt.fit(X_train, y_train)

# Best hyperparameters from the grid search
best_params_dt = grid_search_dt.best_params_
print("Best Hyperparameters for Decision Tree:", best_params_dt)

# Train the final Decision Tree model with the best hyperparameters
final_dt_classifier = DecisionTreeClassifier(**best_params_dt, random_state=42)
final_dt_classifier.fit(X_train, y_train)

# Predict classes on the testing dataset
y_pred_dt = final_dt_classifier.predict(X_test)

# Evaluate the final Decision Tree model on the testing dataset
report_dt = classification_report(y_test, y_pred_dt)
print("Classification Report for Decision Tree on Testing Data:")
print(report_dt)

"""Define the Random Forest classifier"""

from sklearn.ensemble import RandomForestClassifier

rf_classifier = RandomForestClassifier(random_state=42)

# Hyperparameter grid to search
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# Grid search with cross-validation on the training data
grid_search_rf = GridSearchCV(estimator=rf_classifier, param_grid=param_grid_rf, cv=3)
grid_search_rf.fit(X_train, y_train)

# Best hyperparameters from the grid search
best_params_rf = grid_search_rf.best_params_
print("Best Hyperparameters for Random Forest:", best_params_rf)

# Train the final Random Forest model with the best hyperparameters
final_rf_classifier = RandomForestClassifier(**best_params_rf, random_state=42)
final_rf_classifier.fit(X_train, y_train)

# Predict classes on the testing dataset
y_pred_rf = final_rf_classifier.predict(X_test)

# Evaluate the final Random Forest model on the testing dataset
report_rf = classification_report(y_test, y_pred_rf)
print("Classification Report for Random Forest on Testing Data:")
print(report_rf)

"""Define the Multinomial Naive Bayes classifier"""

from sklearn.naive_bayes import MultinomialNB

nb_classifier = MultinomialNB()

# Train the Multinomial Naive Bayes model
nb_classifier.fit(X_train, y_train)

# Predict classes on the testing dataset
y_pred_nb = nb_classifier.predict(X_test)

# Evaluate the Multinomial Naive Bayes model on the testing dataset
report_nb = classification_report(y_test, y_pred_nb)
print("Classification Report for Multinomial Naive Bayes on Testing Data:")
print(report_nb)

"""Create the ensemble model"""

from sklearn.ensemble import VotingClassifier
# from sklearn.ensemble import GradientBoostingClassifier
# from xgboost import XGBClassifier

svm_classifier = SVC(C=1, kernel='linear', random_state=42)
dt_classifier = DecisionTreeClassifier(max_depth=None, min_samples_leaf=1, min_samples_split=10, random_state=42)
rf_classifier = RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300, random_state=42)
# gradient_boosting = GradientBoostingClassifier()

# Create the ensemble model using VotingClassifier
ensemble_classifier = VotingClassifier(estimators=[
    # ('gb', gradient_boosting),
    ('svm', svm_classifier),
    ('decision_tree', dt_classifier),
    ('random_forest', rf_classifier)
], voting='hard')

# Train
ensemble_classifier.fit(X_train, y_train)

# Predict classes on the testing dataset
y_pred_ensemble = ensemble_classifier.predict(X_test)

# Evaluate on the testing dataset
report_ensemble = classification_report(y_test, y_pred_ensemble)
print("Classification Report for Ensemble Model on Testing Data:")
print(report_ensemble)

"""Class 0: Negative sentiment

Class 1: Neutral sentiment

Class 2: Positive sentiment
"""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

conf_matrix_ensemble = confusion_matrix(y_test, y_pred_ensemble)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_ensemble, annot=True, fmt='d', cmap='Blues', cbar=False,
            annot_kws={"size": 12}, linewidths=0.5)
plt.title("Ensemble Model Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""True, Positive = model correctly predicted the class
[192,873,753]

False, Positive = model made a type I error by incorrectly classifying something as positive or neutral
[11,21]

False, Negative = model failed to recognize a positive or neutral sentiment
[40,30,28,52]
"""